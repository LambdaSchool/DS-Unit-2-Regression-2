{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter_optimization_cross_validation_LIVE_LESSON.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O67uhlT4MExK"
      },
      "source": [
        "_Lambda School Data Science — Regression 2_ \n",
        "\n",
        "This sprint, your project is Caterpillar Tube Pricing: Predict the prices suppliers will quote for industrial tube assemblies.\n",
        "\n",
        "# Cross-Validation, Hyperparameter Optimization 🚜\n",
        "\n",
        "\n",
        "### Objectives\n",
        "- Do cross-validation with independent test set\n",
        "- Use scikit-learn for hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTI5WqnGv2b2",
        "colab_type": "text"
      },
      "source": [
        "### Install libraries\n",
        "\n",
        "We will continue to use [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding) and [xgboost](https://xgboost.readthedocs.io/en/latest/).\n",
        "\n",
        "\n",
        "#### category_encoders\n",
        "- Anaconda: `conda install -c conda-forge category_encoders`\n",
        "- Google Colab: `pip install category_encoders`\n",
        "\n",
        "#### xgboost\n",
        "- Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
        "- Windows: `conda install -c anaconda py-xgboost`\n",
        "- Google Colab: already installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsJ7WePuv2b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment & run for Google Colab\n",
        "# !pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh0yfm-0v2b5",
        "colab_type": "text"
      },
      "source": [
        "### Get data\n",
        "\n",
        "We will continue to use the Caterpillar dataset.\n",
        "\n",
        "#### Option 1. Kaggle web UI\n",
        " \n",
        "Sign in to Kaggle and go to the [Caterpillar Tube Pricing](https://www.kaggle.com/c/caterpillar-tube-pricing) competition. Go to the Data page. After you have accepted the rules of the competition, use the download buttons to download the data.\n",
        "\n",
        "\n",
        "#### Option 2. Kaggle API\n",
        "\n",
        "Follow these [instructions](https://github.com/Kaggle/kaggle-api).\n",
        "\n",
        "#### Option 3. Google Drive\n",
        "\n",
        "Download [zip file](https://drive.google.com/uc?export=download&id=1oGky3xR6133pub7S4zIEFbF4x1I87jvC) from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHdra08yv2b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment & run for Option 3 on Google Colab\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COK_XFdqv2b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip caterpillar-tube-pricing.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiVjpMN7v2b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INGXrsisSXFo",
        "colab_type": "text"
      },
      "source": [
        "### Wrangle data\n",
        "\n",
        "This code is similar to what you've seen in  previous notebooks this sprint. We will continue to do the same kind of data wrangling, to prepare our data for cross-validation and hyperparameter optimization. You will likely engineer more features than this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRHBQvDCv2b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Read data\n",
        "train = pd.read_csv('competition_data/train_set.csv')\n",
        "test = pd.read_csv('competition_data/test_set.csv')\n",
        "tube = pd.read_csv('competition_data/tube.csv')\n",
        "materials = pd.read_csv('competition_data/bill_of_materials.csv')\n",
        "components = pd.read_csv('competition_data/components.csv')\n",
        "comp = pd.concat((pd.read_csv(path) for path in glob('competition_data/comp_*.csv')), sort=False)\n",
        "\n",
        "# Get a tidy list of the component types in each tube assembly\n",
        "assembly_components = materials.melt(id_vars='tube_assembly_id', \n",
        "                                     value_vars=[f'component_id_{n}' for n in range(1,9)])\n",
        "\n",
        "assembly_components = (assembly_components\n",
        "                       .sort_values(by='tube_assembly_id')\n",
        "                       .dropna()\n",
        "                       .rename(columns={'value': 'component_id'}))\n",
        "\n",
        "assembly_component_types = assembly_components.merge(components, how='left')\n",
        "\n",
        "# Make a crosstab of the component types for each assembly (one-hot encoding)\n",
        "table = pd.crosstab(assembly_component_types['tube_assembly_id'], \n",
        "                    assembly_component_types['component_type_id'])\n",
        "table = table.reset_index()\n",
        "\n",
        "# Get features for each component\n",
        "features = ['component_id', 'component_type_id', 'orientation', 'unique_feature', 'weight']\n",
        "comp = comp[features]\n",
        "comp['orientation'] = (comp['orientation']=='Yes').astype(int)\n",
        "comp['unique_feature'] = (comp['unique_feature']=='Yes').astype(int)\n",
        "comp['weight'] = comp['weight'].fillna(comp['weight'].median())\n",
        "\n",
        "# Get aggregated features for all components in each tube assembly.\n",
        "# This code is a little complex, but we discussed in detail last lesson.\n",
        "materials['components_total'] = sum(materials[f'quantity_{n}'].fillna(0)  for n in range(1,9))\n",
        "materials['components_distinct'] = sum(materials[f'component_id_{n}'].notnull().astype(int) for n in range(1,9))\n",
        "materials['orientation'] = 0\n",
        "materials['unique_feature'] = 0\n",
        "materials['weight'] = 0\n",
        "\n",
        "for n in range(1,9):\n",
        "    materials = materials.merge(comp, how='left', \n",
        "                                left_on=f'component_id_{n}', \n",
        "                                right_on='component_id', \n",
        "                                suffixes=('', f'_{n}'))\n",
        "\n",
        "for col in materials:\n",
        "    if 'orientation' in col or 'unique_feature' in col or 'weight' in col:\n",
        "        materials[col] = materials[col].fillna(0)\n",
        "        \n",
        "materials['orientation'] = sum(materials[f'orientation_{n}'] for n in range(1,9))\n",
        "materials['unique_feature'] = sum(materials[f'unique_feature_{n}'] for n in range(1,9))\n",
        "materials['weight'] = sum(materials[f'weight_{n}'] for n in range(1,9))\n",
        "\n",
        "features = ['tube_assembly_id', 'orientation', 'unique_feature', 'weight', \n",
        "            'components_total', 'components_distinct', 'component_id_1']\n",
        "materials = materials[features]\n",
        "\n",
        "# Extract year from quote date\n",
        "train['quote_date_year'] = pd.to_datetime(train['quote_date'], infer_datetime_format=True).dt.year\n",
        "test['quote_date_year'] = pd.to_datetime(train['quote_date'], infer_datetime_format=True).dt.year\n",
        "\n",
        "# Merge data\n",
        "train = (train\n",
        "         .merge(tube, how='left')\n",
        "         .merge(materials, how='left')\n",
        "         .merge(table, how='left')\n",
        "         .fillna(0))\n",
        "\n",
        "test = (test\n",
        "        .merge(tube, how='left')\n",
        "        .merge(materials, how='left')\n",
        "        .merge(table, how='left')\n",
        "        .fillna(0))\n",
        "\n",
        "# Arrange X matrix and y vector.\n",
        "# Drop `tube_assembly_id` because our goal is to predict unknown assemblies,\n",
        "# and no tube assembly id's are shared between the train and test sets.\n",
        "target = 'cost'\n",
        "features = train.columns.drop([target, 'tube_assembly_id'])\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_test = test[features]\n",
        "\n",
        "# Log-transform the target\n",
        "y_train_log = np.log1p(y_train)\n",
        "\n",
        "# Make pipeline\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwrvaIwzv2cA",
        "colab_type": "text"
      },
      "source": [
        "## Do cross-validation with independent test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr-Dt67Gv2cB",
        "colab_type": "text"
      },
      "source": [
        "Let's take another look at [Sebastian Raschka's diagram of model evaluation methods.](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html) So far we've been using \"**train/validation/test split**\", but we have more options. \n",
        "\n",
        "Today we'll learn about \"k-fold **cross-validation** with independent test set\", for \"model selection (**hyperparameter optimization**) and performance estimation.\"\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg\" width=\"600\">\n",
        "\n",
        "<sup>Source: https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html</sup>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozuzFo_Pv2cB",
        "colab_type": "text"
      },
      "source": [
        "The Scikit-Learn docs show a diagram of how k-fold cross-validation works, and explain the pros & cons of cross-validation versus train/validate/test split.\n",
        "\n",
        "#### [Scikit-Learn User Guide, 3.1 Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "\n",
        "> When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
        "\n",
        "> However, **by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.**\n",
        "\n",
        "> **A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV.** \n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"600\">\n",
        "\n",
        "> In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k “folds”:\n",
        "\n",
        "> - A model is trained using $k-1$ of the folds as training data;\n",
        "> - the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
        "\n",
        "> The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. **This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o10EvckQv2cC",
        "colab_type": "text"
      },
      "source": [
        "### cross_val_score\n",
        "\n",
        "How do we get started? According to the [Scikit-Learn User Guide](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics), \n",
        "\n",
        "> The simplest way to use cross-validation is to call the [**`cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) helper function\n",
        "\n",
        "However, this isn't _so_ simple with the Caterpillar dataset, because:\n",
        "\n",
        "- We want all rows for a given `tube_assembly_id` to go into the same \"fold.\" (Why? [See the discussion here](https://www.fast.ai/2017/11/13/validation-sets/) under _\"New people, new boats\"_ for a reminder.) We can do this with the `cross_val_score` function, using its `groups` parameter.\n",
        "- For scikit-learn's cross-validation [**scoring**](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), higher is better. But for regression error metrics, lower is better. So scikit-learn multiplies regression error metrics by -1 to make them negative. That's why the value of the `scoring` parameter is `'neg_mean_squared_error'`. \n",
        "- Scikit-learn doesn't implement RMSE, so we take the square root of MSE. First, we must multiply the scores by -1 to make them positive.\n",
        "- RMSE with the log-transformed target is equivalent to RMSLE with the original target.\n",
        "\n",
        "Put it all together, and k-fold cross-validation with the Caterpillar dataset looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-TqB5Hsv2cC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k = 3\n",
        "groups = train['tube_assembly_id']\n",
        "scores = cross_val_score(pipeline, X_train, y_train_log, cv=k, \n",
        "                         scoring='neg_mean_squared_error', groups=groups)\n",
        "print(f'RMSLE for {k} folds:', np.sqrt(-scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dq-PfpGZSHJ",
        "colab_type": "text"
      },
      "source": [
        "But the Random Forest has many hyperparameters. We mostly used the defaults, and arbitrarily chose `n_estimators`. Is it too high? Too low? Just right? How do we know?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCubg7EbjZyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Model Hyperparameters:')\n",
        "print(pipeline.named_steps['randomforestregressor'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6o8W7Cv2cE",
        "colab_type": "text"
      },
      "source": [
        "\"The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\" —[Francois Chollet](https://books.google.com/books?id=dadfDwAAQBAJ&pg=PA114)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8uKvR_pv2cG",
        "colab_type": "text"
      },
      "source": [
        "### Validation Curve\n",
        "\n",
        "Let's try different parameter values, and visualize \"the border between underfitting and overfitting.\" \n",
        "\n",
        "Using scikit-learn, we can make [validation curves](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html), \"to determine training and test scores for varying parameter values. This is similar to grid search with one parameter.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEIxeNXdv2cF",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png\">\n",
        "\n",
        "<sup>Source: https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn</sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3bbgaP2c3Pr",
        "colab_type": "text"
      },
      "source": [
        "Validation curves are awesome for learning about overfitting and underfitting. (But less useful in real-world projects, because we usually want to vary more than one parameter.)\n",
        "\n",
        "For this example, let's see what happens when we vary the depth of a decision tree. (This will be faster than varying the number of estimators in a random forest.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znIz2FPQv2cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "pipeline = make_pipeline(ce.OrdinalEncoder(), DecisionTreeRegressor())\n",
        "\n",
        "depth = range(1, 15, 2)\n",
        "train_scores, val_scores = validation_curve(\n",
        "    pipeline, X_train, y_train_log,\n",
        "    param_name='decisiontreeregressor__max_depth', \n",
        "    param_range=depth, scoring='neg_mean_squared_error', \n",
        "    cv=2, groups=groups)\n",
        "\n",
        "train_rmsle = np.sqrt(-train_scores)\n",
        "val_rmsle = np.sqrt(-val_scores)\n",
        "plt.plot(depth, np.mean(train_rmsle, axis=1), color='blue', label='training error')\n",
        "plt.plot(depth, np.mean(val_rmsle, axis=1), color='red', label='validation error')\n",
        "plt.xlabel('depth')\n",
        "plt.ylabel('RMSLE')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUaLgk8Pv2cJ",
        "colab_type": "text"
      },
      "source": [
        "## Use scikit-learn for hyperparameter optimization\n",
        "\n",
        "To vary multiple hyperparameters and find their optimal values, let's try **Randomized Search CV.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AexbC7fjv2cL",
        "colab_type": "text"
      },
      "source": [
        "#### [Scikit-Learn User Guide, 3.2 Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html)\n",
        "\n",
        "> Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. \n",
        "\n",
        "> While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values.\n",
        "\n",
        "> Specifying how parameters should be sampled is done using a dictionary. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZeZd0RsWZL",
        "colab_type": "text"
      },
      "source": [
        "For the sake of time, let's just do 5 iterations of randomized search, with 2-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtZQbJQ5v2cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import randint, uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    RandomForestRegressor(random_state=42)\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    'randomforestregressor__n_estimators': randint(50, 500), \n",
        "    'randomforestregressor__max_features': uniform(), \n",
        "    'randomforestregressor__min_samples_leaf':  [1, 10, 100]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    pipeline, \n",
        "    param_distributions=param_distributions, \n",
        "    n_iter=5, \n",
        "    cv=2, \n",
        "    scoring='neg_mean_squared_error', \n",
        "    verbose=10, \n",
        "    return_train_score=True, \n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train_log, groups=groups);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9M-OOJltM_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best hyperparameters', search.best_params_)\n",
        "print('Cross-validation RMSLE', np.sqrt(-search.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo9-Kbx6uWM3",
        "colab_type": "text"
      },
      "source": [
        "The score may be underwhelming to you, but it's just a demo. Try it after the lesson, with all your features, for more iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q79ipvpgqYwF",
        "colab_type": "text"
      },
      "source": [
        "### \"Fitting X folds for each of Y candidates, totalling Z fits\" ?\n",
        "\n",
        "What did that mean? What do you think?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjXNObHuTXx",
        "colab_type": "text"
      },
      "source": [
        "### Do it with xgboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FabSX50trkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    XGBRegressor(random_state=42)\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    'xgbregressor__n_estimators': randint(500, 1000), \n",
        "    'xgbregressor__max_depth': randint(3, 7)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    pipeline, \n",
        "    param_distributions=param_distributions, \n",
        "    n_iter=5, \n",
        "    cv=2, \n",
        "    scoring='neg_mean_squared_error', \n",
        "    verbose=10, \n",
        "    return_train_score=True, \n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train_log, groups=groups);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9NJ7deQuxCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best hyperparameters', search.best_params_)\n",
        "print('Cross-validation RMSLE', np.sqrt(-search.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tJr3YZ8xLt-",
        "colab_type": "text"
      },
      "source": [
        "### See detailed results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGHRUlY3xF1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDZyu6FNyY2l",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuWqQUk_yIw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = search.best_estimator_\n",
        "y_pred_log = pipeline.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log)  # Convert from log-dollars to dollars\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission['cost'] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLh5dBMa7d2c",
        "colab_type": "text"
      },
      "source": [
        "# ASSIGNMENT\n",
        "- Use the Caterpillar dataset (or _any_ dataset of your choice). \n",
        "- Use scikit-learn for hyperparameter optimization with RandomSearchCV.\n",
        "- Add comments and Markdown to your notebook. Clean up your code.\n",
        "- Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "### Stretch Goals\n",
        "- Make your final Kaggle submissions. Improve your scores! Look at [Kaggle Kernels](https://www.kaggle.com/c/caterpillar-tube-pricing/kernels) for ideas. **Share your best features and techniques on Slack.**\n",
        "- In additon to `RandomizedSearchCV`, scikit-learn has [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
        "- _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
        "\n",
        "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
        "\n",
        "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n",
        "\n",
        "### Post-Reads\n",
        "- Jake VanderPlas, [_Python Data Science Handbook_, Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), Hyperparameters and Model Validation\n",
        "- Jake VanderPlas, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=107)\n",
        "- Ron Zacharski, [_A Programmer's Guide to Data Mining_, Chapter 5](http://guidetodatamining.com/chapter5/), 10-fold cross validation\n",
        "- Sebastian Raschka, [A Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sQiv9s2kOjn",
        "colab_type": "text"
      },
      "source": [
        "## Try adjusting these hyperparameters in your future projects\n",
        "\n",
        "### Tree ensembles\n",
        "\n",
        "#### Random Forest\n",
        "- class_weight (for imbalanced classes)\n",
        "- max_depth (usually high)\n",
        "- max_features (decrease for more variance)\n",
        "- min_samples_leaf (increase if overfitting)\n",
        "- n_estimators (too low underfits, too high wastes time)\n",
        "\n",
        "#### Xgboost\n",
        "- scale_pos_weight (for imbalanced classes)\n",
        "- max_depth (usually low)\n",
        "- n_estimators (too low underfits, too high overfits)\n",
        "\n",
        "For more ideas, see [Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) and [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html).\n",
        "\n",
        "### Linear models\n",
        "\n",
        "#### Logistic Regression\n",
        "- C\n",
        "- class_weight (for imbalanced classes)\n",
        "- penalty\n",
        "\n",
        "#### Ridge / Lasso Regression\n",
        "- alpha\n",
        "\n",
        "#### ElasticNet Regression\n",
        "- alpha\n",
        "- l1_ratio\n",
        "\n",
        "For more explanation, see [**Aaron Gallant's 9 minute video on Ridge Regression**](https://www.youtube.com/watch?v=XK5jkedy17w)!\n"
      ]
    }
  ]
}